# Red Snapper larval physical modeling #

################### Modeling ###################
#                      TACC                     #
# https://www.tacc.utexas.edu/systems/stampede2 #

# Each year run through this code separately

YEAR=2017
##### Making $YEAR folder #####
echo $YEAR
cd $SCRATCH
cd $YEAR
mkdir input
cp $WORK/CMS/input/* input

#Convert stored nest file
awk -v YEAR=$YEAR 'NR==13{gsub(2016,YEAR,$0); print $0; next}NR==16{gsub(2016,YEAR,$0); print $0; next}NR==15{gsub(2,1,$0); print $0; next}{print $0}' input/nest_1.nml > input/tmp
mv input/tmp input/nest_1.nml

#Converting nest files from getdata cmds (2017 nest2 example)
sed '/filename/d' nest_2.nml | sed 's/-30/1.2676506E30/g' > input/nest_1.nml

#####Making 50 nest files so a release point can be run in 2 goes##### (~30 mins)
for i in $(seq 1 50); do cp -r $SCRATCH/data/expt_getdata_GOM$YEAR/nests $SCRATCH/$YEAR/nests_$i; done

#Making the directories for each release date
echo -e May15 | while read j; do
YEAR=2017
MONTH=(`grep -w $j $SCRATCH/data/var_set | awk '{print $2}'`)
DAY=(`grep -w $j $SCRATCH/data/var_set | awk '{print $3}'`)
echo $j $MONTH $DAY $YEAR
mkdir $SCRATCH/$YEAR/$j
cd $SCRATCH/$YEAR/$j
cp $SCRATCH/data/locs/GOM_grid_all_releaseFile .
split -d -a 4 -l 1 GOM_grid_all_releaseFile group.
rm GOM_grid_all_releaseFile
ls group* > file.list
split -d -a 4 -l 54 file.list files.
cp $WORK/bin/slurm/CMS.slurm .

k=0
m=1
for i in $(ls group*); do
k=$(echo "$k+1"| bc)
if [ $k == 55 ]; then k=1; m=$(echo "$m+1"| bc); fi
if [ $m == 51 ]; then m=1; fi
mkdir input_$i expt_$i; ln -s /scratch/07285/afields3/$YEAR/nests_$m expt_$i/nests
cp /scratch/07285/afields3/$YEAR/input/* input_$i/
awk -v a="$MONTH" -v b="$DAY" -v c="$YEAR" '{print $0, c, a, b, "0"}' $i > input_$i/release.file;
done
cd ..
done
}

{#Testing May15 modeling
ls files.* | head -n 1 | while read i; do
sed -i "s/files.[0-9][0-9][0-9][0-9]/$i/g" CMS.slurm
sbatch -J CMS_$i -o CMS_$i.out -e CMS_$i.err -t 03:00:00 CMS.slurm
done

#Checking test
less expt_group.0000/output/traj_file_1

}

{#Old way of doing each release date
#First run in a release date
ls files.* | head -n 50 | while read i; do
sed -i "s/files.[0-9][0-9][0-9][0-9]/$i/g" CMS.slurm
sbatch -J CMS_$i -o CMS_$i.out -e CMS_$i.err -t 03:00:00 CMS.slurm
done

#Second run in a release date
ls files.* | tail -n +51 | head -n 50 | while read i; do
sed -i "s/files.[0-9][0-9][0-9][0-9]/$i/g" CMS.slurm
sbatch -J CMS_$i -o CMS_$i.out -e CMS_$i.err -t 03:00:00 CMS.slurm
done
}

{#Running May15 modeling
#Getting all of the files run on a loop
echo -e May15 | while read j; do
cd $j
echo "Starting $j"
ls files.* | while read i; do
while [ $(squeue -u afields3 | grep afields | wc -l) -gt 45 ]; do sleep 60; done
sed -i "s/files.[0-9][0-9][0-9][0-9]/$i/g" CMS.slurm
sbatch -J CMS_$i -o CMS_$i.out -e CMS_$i.err -t 01:00:00 CMS.slurm
sleep 10
done || exit 1
cd ..
done
}

{#Adding May15 to cat folder
echo -e May15 | while read k; do
echo $k
cd $k
for i in $(ls group*); do
POLY=$(cut -d" " -f1 $i)
cp expt_$i/output/traj_file_1 ../cat/traj_file_${k}_${i}
awk -v poly=$POLY '{$1=""; print poly $0}' ../cat/traj_file_${k}_${i} > ../cat/tmp; mv ../cat/tmp ../cat/traj_file_${k}_${i}
done
cd ..
done

tar -cf May15_2019.tar cat/*May15*
}

#Notebook cleanup
}}}}}}}}}}}}}}}}}}

################### Data Transfer ###################
#DT

{#Sending nest files to TACC for modeling
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2011
scp input_getdata_GOM2011/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2011
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2012
scp input_getdata_GOM2012/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2012
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2013
scp input_getdata_GOM2013/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2013
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2014
scp input_getdata_GOM2014/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2014
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2017
scp input_getdata_GOM2017/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2017
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2018
scp input_getdata_GOM2018/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2018
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/2019
scp input_getdata_GOM2019/nest_1.nml afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2019
}

#HPC
{#Moving tarballs to HPC
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2010/May15_2010.tar .
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2011/May15_2011.tar .
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2012/May15_2012.tar .
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2013/May15_2013.tar .
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2014/May15_2014.tar .
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2018/May15_2018.tar .
scp afields3@stampede2.tacc.utexas.edu:/scratch/07285/afields3/2019/May15_2019.tar .
}

#Notebook cleanup
}}}}}}}}}}}}}}}}}}

################### Model analysis ###################
#DT
{#Method 6 without grid shp file
#Libraries
library(raster)
library(rgdal)
library(sf)
library(spatialEco)

#Pull in all the grids
net <- read.table("../Zach/GOM_Polygons_all.txt", head=F)
names(net) <- c("Longitude","Latitude","Polygon")
net <- unique(net)

# Start the clock!
ptm <- proc.time()

#Put each polygon into a list
tmp <- list()
for(i in 1:max(net$Polygon)){
tmp.file <- Polygon(net[net$Polygon==i, c('Longitude','Latitude')])
tmp[[i]] <- Polygons(list(tmp.file), ID = i)
}

# Stop the clock
proc.time() - ptm
#   user  system elapsed
# 23.623   0.176  23.801

#Convert to SpatialPolygonsDataFrame
SPs <- SpatialPolygons(tmp)
SPDF <- SpatialPolygonsDataFrame(SPs, data.frame(N = unique(net$Polygon)))
proj4string(SPDF) <- CRS("+init=epsg:3160")
#plot(SPDF)		#Takes a long time

#Output file for future use
writeOGR(SPDF, dsn = getwd(), layer = "GOM_Polygons_all", driver="ESRI Shapefile")
#Read shapefile
#poly <- readOGR()

#Make release file
release <- read.table("tmp.out",head=F)
colnames(release) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
release$Longitude <- release$Longitude -360

# prepare coordinates, data, and proj4string
coords <- release[ , c('Longitude','Latitude')]   																# coordinates
data   <- release[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")]          # data
crs    <- CRS("+init=epsg:3160") 																				# proj4string of coords

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= data, proj4string = crs)

new_shape <- point.in.poly(pts, SPDF)
tmp.dat <- cbind(as.data.frame(pts@coords), new_shape)
write.table(tmp.dat, "test.txt", col.names=T, row.names=F, quote=F)

}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}
{#Method 6 with grid shape
#Libraries
library(raster)
library(rgdal)
library(sf)
library(spatialEco)

#Read shapefile
polys <- readOGR(dsn=getwd(), layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))

#Make release file
release <- read.table("tmp.out",head=F)
colnames(release) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(release$Longitude[1] > 0){release$Longitude <- release$Longitude -360}

# prepare coordinates, data, and proj4string
coords <- release[ , c('Longitude','Latitude')]
data   <- release[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")]
crs    <- CRS("+init=epsg:3160")

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= data, proj4string = crs)

new_shape <- point.in.poly(pts, polys)
tmp.dat <- cbind(as.data.frame(pts@coords), new_shape)
write.table(tmp.dat, "test.txt", col.names=T, row.names=F, quote=F)
}}}}}}}}}}}}}
{#Making Method 6 into an R script										#Polywoggle
#!/bin/R
#Usage: Rscript Polywoggle.R <particle.data> <Output.txt>

#Get arguements
args <- commandArgs(trailingOnly=TRUE)
RELEASE <- args[1]
OUT <- args[2]

#Libraries
suppressMessages(library(raster))
suppressMessages(library(rgdal))
suppressMessages(library(sf))
suppressMessages(library(spatialEco))

#Read shapefile
polys <- readOGR(dsn=getwd(), layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))

#Make release file
release <- read.table(RELEASE,head=F)
colnames(release) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(release$Longitude[1] > 0){release$Longitude <- release$Longitude -360}

# prepare coordinates, data, and proj4string
coords <- release[ , c('Longitude','Latitude')]
dat   <- release[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")]
crs    <- CRS("+init=epsg:3160")

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= dat, proj4string = crs)

#Informing user of progress
print("Data loaded scuccessfully")
print("Comparing files")

#Look for intersection
new_shape <- point.in.poly(pts, polys)

#Combine with location data
tmp.dat <- cbind(as.data.frame(pts@coords), new_shape@data)

#Output the results
write.table(tmp.dat, OUT, col.names=T, row.names=F, quote=F, sep="\t")
}
}
{#Testing
tail -n10000 traj_file_Aug15_group.0000 | awk -v FS=" " '$3==2419200{print $0}' > Aug15_g0.txt
time Rscript Polywoggle.R Aug15_g0.txt try1.txt
}
{#Making Method 6 into an R script which will process an entire date	#MultiPolywoggle
#!/bin/R
#Usage: Rscript MultiPolywoggle.R <path to files> <particle.data prefix>

#Get arguements
args <- commandArgs(trailingOnly=TRUE)
PATH <- args[1]
PATTERN <- args[2]

#Libraries
suppressMessages(library(raster))
suppressMessages(library(rgdal))
suppressMessages(library(sf))
suppressMessages(library(spatialEco))

#Read shapefile
polys <- readOGR(dsn=getwd(), layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))

#Get files list
files <- list.files(path=PATH,pattern=PATTERN)
files <- files[grep("intersect",files,invert=T)]

#print(c(PATH, PATTERN, files))

for(i in files){
print(processing i)
RELEASE <- i

#Make release file
release <- read.table(RELEASE,head=F)
colnames(release) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(release$Longitude[1] > 0){release$Longitude <- release$Longitude -360}

# prepare coordinates, data, and proj4string
coords <- release[ , c('Longitude','Latitude')]
dat   <- release[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")]
crs    <- CRS("+init=epsg:3160")

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= dat, proj4string = crs)

#Look for intersection
new_shape <- point.in.poly(pts, polys)

#Combine with location data
tmp.dat <- cbind(as.data.frame(pts@coords), new_shape@data)

#Output the results
write.table(tmp.dat, paste(i,"intersect",sep="."), col.names=T, row.names=F, quote=F, sep="\t")
}
}

}
{#Preparing to inspect particle files
#Get particle data
dat1 <- read.table("Aug15_26_g0.txt",head=F)
colnames(dat1) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(dat1$Longitude[1] > 0){dat1$Longitude <- dat1$Longitude -360}
dat2 <- read.table("Aug15_27_g0.txt",head=F)
colnames(dat2) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(dat2$Longitude[1] > 0){dat2$Longitude <- dat2$Longitude -360}
dat3 <- read.table("Aug15_28_g0.txt",head=F)
colnames(dat3) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(dat3$Longitude[1] > 0){dat3$Longitude <- dat3$Longitude -360}
dat0 <- read.table("Aug15_15_g0.txt",head=F)
colnames(dat0) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(dat0$Longitude[1] > 0){dat0$Longitude <- dat0$Longitude -360}

MIN <- min(c(dat0$Distance,dat1$Distance,dat2$Distance,dat3$Distance))
MAX <- max(c(dat0$Distance,dat1$Distance,dat2$Distance,dat3$Distance))

par(mfrow=c(4,1))
hist(dat0$Distance, breaks=40, xlim=c(MIN,MAX))
hist(dat1$Distance, breaks=40, xlim=c(MIN,MAX))
hist(dat2$Distance, breaks=40, xlim=c(MIN,MAX))
hist(dat3$Distance, breaks=40, xlim=c(MIN,MAX))

mean(dat0$Distance)
mean(dat1$Distance)
mean(dat2$Distance)
mean(dat3$Distance)

median(dat0$Distance)
median(dat1$Distance)
median(dat2$Distance)
median(dat3$Distance)

library(sp)
# make the SpatialPointsDataFrame object
pts0 <- SpatialPointsDataFrame(coords = dat0[ , c('Longitude','Latitude')], data= dat0[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")], proj4string = CRS("+init=epsg:3160"))
pts1 <- SpatialPointsDataFrame(coords = dat1[ , c('Longitude','Latitude')], data= dat1[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")], proj4string = CRS("+init=epsg:3160"))
pts2 <- SpatialPointsDataFrame(coords = dat2[ , c('Longitude','Latitude')], data= dat2[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")], proj4string = CRS("+init=epsg:3160"))
pts3 <- SpatialPointsDataFrame(coords = dat3[ , c('Longitude','Latitude')], data= dat3[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")], proj4string = CRS("+init=epsg:3160"))

XMIN <- min(c(pts0@coords[,1], pts1@coords[,1], pts2@coords[,1], pts3@coords[,1], -84.436))*1.001
XMAX <- max(c(pts0@coords[,1], pts1@coords[,1], pts2@coords[,1], pts3@coords[,1], -84.436))*0.999
YMIN <- min(c(pts0@coords[,2], pts1@coords[,2], pts2@coords[,2], pts3@coords[,2], 27.411))*0.999
YMAX <- max(c(pts0@coords[,2], pts1@coords[,2], pts2@coords[,2], pts3@coords[,2], 27.411))*1.001

par(mfrow=c(4,1))
plot(pts0, pch=20, col="mediumblue", xlim=c(XMIN,XMAX), ylim=c(YMIN,YMAX))
points(-84.436,27.411,pch="*", col="red4", cex=2)
plot(pts1, pch=20, col="mediumblue", xlim=c(XMIN,XMAX), ylim=c(YMIN,YMAX))
points(-84.436,27.411,pch="*", col="red4", cex=2)
plot(pts2, pch=20, col="mediumblue", xlim=c(XMIN,XMAX), ylim=c(YMIN,YMAX))
points(-84.436,27.411,pch="*", col="red4", cex=2)
plot(pts3, pch=20, col="mediumblue", xlim=c(XMIN,XMAX), ylim=c(YMIN,YMAX))
points(-84.436,27.411,pch="*", col="red4", cex=2)

START<-data.frame(matrix(c(-84.436,27.411),ncol=2))

library(usmap)
library(ggplot2)

test.start <- usmap_transform(START)
test.pts0 <- usmap_transform(as.data.frame(pts0@coords))
test.pts1 <- usmap_transform(as.data.frame(pts1@coords))
test.pts2 <- usmap_transform(as.data.frame(pts2@coords))
test.pts3 <- usmap_transform(as.data.frame(pts3@coords))

p0 <- plot_usmap(include = c("FL")) +
  geom_point(data = test.pts0, aes(x = Longitude.1, y = Latitude.1), color = "mediumblue") +
  geom_point(data = test.start, aes(x = X1.1, y = X2.1), color = "red") +
  labs(title = "Day 15") +
  theme(legend.position = "right")

p1 <- plot_usmap(include = c("FL")) +
  geom_point(data = test.pts1, aes(x = Longitude.1, y = Latitude.1), color = "mediumblue") +
  geom_point(data = test.start, aes(x = X1.1, y = X2.1), color = "red") +
  labs(title = "Day 26") +
  theme(legend.position = "right")

p2 <- plot_usmap(include = c("FL")) +
  geom_point(data = test.pts2, aes(x = Longitude.1, y = Latitude.1), color = "mediumblue") +
  geom_point(data = test.start, aes(x = X1.1, y = X2.1), color = "red") +
  labs(title = "Day 27") +
  theme(legend.position = "right")

p3 <- plot_usmap(include = c("FL")) +
  geom_point(data = test.pts3, aes(x = Longitude.1, y = Latitude.1), color = "mediumblue") +
  geom_point(data = test.start, aes(x = X1.1, y = X2.1), color = "red") +
  labs(title = "Day 28") +
  theme(legend.position = "right")

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

}}#http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

}}}}}}}}}}}}
{#Getting the locations of the caught fish
cd ~/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/
nano Caught_locs.txt
{#Data for file
lon	lat
-96.947467	27.3532
-88.87021667	29.20453333
-85.74845	29.16303333
-84.38	28.35
-84.06	26.59
-84.06	29.09
-84.41	28.59
-85.57	28.49
-83.13	25.42
-84.0049	25.1766
-84.0174	27.2179
-85.0801	28.5497
-84.4868	28.5335
-83.1945	26.5334
-84.383	28.066
-83.388	28.0284
-83.4	25.49
-92.6318	28.42023333
-93.2856	27.89746667
-91.6533	27.98025
-91.65873333	27.9906
-92.66408333	28.44385
-93.28673333	27.89333333
-91.65123333	27.98318333
-91.65911667	27.9879
-91.65301667	29.98511667
-91.65811667	27.99026667
-93.28245	27.89175
-93.28241667	27.8948
-92.63348333	28.41988333
-92.63348333	28.41988333
-91.65385	27.9875
-93.29228333	27.90226667
-91.60416667	28.4125
-93.51	28.33
-93.58	28.09
-93.35	28.06
-93.28673333	27.89333333
-91.57	28.06
-91.11	28.02
-93.3249	28.0271
-89.19	28.49
-89.55	28.85
-90.47	28.31
-93.3208	28.0397
-93.3235	28.2626
-93.5101	28.3263
-92.41933333	28.60716667
-91.5594	28.0356
-90.1612	28.2098
-90.0945	28.2916
-90.0947	28.2757
-91.5659	28.0573
-84.0591	29.0921
-80.5	28.1
-96.633	25.55
-97.301	24.083
-97.4	22.383
-97.45	22.33
-96.45623333	19.89233333
-96.366	20.016
-96.283	19.9
-95.716	19.166
-94.466	18.61
-92.4002	20.83126667
-91.50273333	22.24951667
-90.03481667	22.5801
-90.11301667	22.66201667
-92.33093333	19.38703333
-88.04	29.45
-88.3091	29.2741
-88.076	29.5105
-85.95038333	30.18888333
-88.1898	29.3853
-85.92503333	30.18776667
-85.86331667	30.0405
-85.86706667	30.0542
-88.1684	29.1298
-85.99526667	29.93878333
-85.99521667	29.93878333
-86.00725	29.92638333
-86.00766667	30.04103333
-86.04036667	30.01006667
-85.9	30.1
-88.21	29.23
-85.93605	30.08645
-85.89648333	30.06766667
-88.0383	29.451
-88.867	29.167
-86.31	29.46
-88.2075	29.229
-86.3091	29.4613
-86.0545	29.5412
-87.5634	29.4763
-86.1708	29.4183
-86.21325	29.95316667
-86.23	30.2
-88.14478333	29.81848333
-88.41076667	29.97131667
-88.03278333	29.74013333
-88.10595	29.84183333
-88.48523333	30.11116667
-88.51275	29.7673
-88.4696	30.04696667
-88.13068333	29.50081667
-88.14063333	29.50448333
-88.07263333	29.46565
-88.48358333	29.68348333
-88.07403333	29.42431667
-87.233	29.85
-88.076	29.5105
-87.117	29.5201
-96.25	26.28
-96.23	27.37
-96.36	26.26
-96.12	27.4
-96.47	26.55
-95.45	27.46
-94.53	28.01
-96.4279	27.1506
-95.14	27.55
-96.98	26.3795
-95.145	28.63166667
-97.0765	27.38183333
-95.0885	28.77283333
-95.086	28.8155
-97.027	26.085
-96.86183333	26.057
-97.1	26.32133333
-97.2215	27.27166667
-97.31	27.16816667
-97.06466667	27.26333333
-96.70066667	27.17783333
-96.94333333	27.2385
-94.42	28.32
-94.03	28.09
-96.4689	26.5451
-96.2254	27.3664
-96.1165	27.4016
-95.4538	27.4599
-76.98333333	34.4
-77.53333333	33.61666667
-77.88333333	33.5
-77.88333333	34
-77.88333333	34.4
-77.66666667	33.25
}

```{R}```
#Libraries
suppressMessages(library(raster))
suppressMessages(library(rgdal))
suppressMessages(library(sf))
suppressMessages(library(spatialEco))

#Read shapefile
polys <- readOGR(dsn=getwd(), layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))

#Make release file
release <- read.table("Caught_locs.txt",head=T)

# prepare coordinates, data, and proj4string
coords <- release[ , c('lon','lat')]
crs    <- CRS("+init=epsg:3160")

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= release, proj4string = crs)

#Look for intersection
new_shape <- point.in.poly(pts, polys)

#Output the results
write.table(new_shape@data, "Caught_grid.txt", col.names=T, row.names=F, quote=F, sep="\t")

library(sp)
library(usmap)
library(ggplot2)
dat1 <- read.table("Caught_grid.txt",head=T)
pts1 <- SpatialPointsDataFrame(coords = dat1[ , c('lon','lat')], data= dat1[ , c("lon", "lat")], proj4string = CRS("+init=epsg:3160"))
test.pts1 <- usmap_transform(as.data.frame(pts1@coords))
plot_usmap(include = c("NC", "SC", "GA", "FL", "AL", "MS","LA", "TX")) +
  geom_point(data = test.pts1, aes(x = lon.1, y = lat.1), color = "mediumblue") +
  labs(title = "Day 26") +
  theme(legend.position = "right")

#https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html
#http://www.sthda.com/english/wiki/ggplot2-themes-and-background-colors-the-3-elements#customize-the-appearance-of-the-plot-background
#Mapping script for Natalie's stuff
library("rnaturalearth")
library("rnaturalearthdata")
library('sf')
library('ggmap')
library("ggspatial")

world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)
world <- ne_countries(scale = "medium", returnclass = "sf")

coord_sf(crs = "+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs ")

#Cartoon map
c1 <- ggplot(data = world) +
	ggtitle("Red Snapper Catch locations") +
	geom_sf(fill = "lightgreen") +
	xlab("Longitude") + ylab("Latitude") +
	annotation_north_arrow(location = "br", which_north = "true", pad_x = unit(0.75, "in"), pad_y = unit(0.5, "in"), style = north_arrow_fancy_orienteering) +
    geom_point(data = test.pts1, aes(x = lon, y = lat), color = "mediumblue") + 
	theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = "skyblue1", color="skyblue1")) + 
    ggplot2::coord_sf(xlim = c(-98, -76.78333), ylim = c(18, 34.7), expand=F)

#Terrain map
GOM_map<-ggmap(get_stamenmap(bbox = c(left = -98, bottom = 18, right =-76.5, top = 34.7), zoom = 9, maptype="terrain"))
t1 <- GOM_map + geom_point(data = test.pts1, aes(x = lon, y = lat), color = "mediumblue")

ggsave("Catch_cartoon_map.tif",c1, device="tiff")
ggsave("Catch_terrain_map.tif",t1, device="tiff")

q("no")
}

#Caught files and shp files put in support directory

### Workspace for making things work ###
{#HPC
{#Making probability model from 2009
#preparing to run the modeling on all the 2009 files
cd $WORK/Workspace/red_snapper/model/TACC/2009
for i in $(seq -w 0 19); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#53340
echo "53340/20" | bc
#2667
split -l 2667 -d filelist group

split -l 2934 -d filelist group

seq -w 0 19 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done

#Preparing the datafiles (pulling all the points wanted)
ls *group* | sed 's/traj_file_//g'| while read i; do
tail -n40000 traj_file_${i} | awk -v FS=" " '$3>=2246400{print $0}' | awk -v FS=" " '$8==0{print $0}' > ${i}.txt
done

#Getting the grid files
cp -s ../support/* .

#Making the intersection analysis
if [ $(squeue | grep jgoldq | wc -l) -eq 0 ]; then sbatch -p jgoldq $WORK/bin/slurm/Polywoggle.slurm
elif [ $(squeue | grep jgoldq | wc -l) -gt 0 ]; then sbatch $WORK/bin/slurm/Polywoggle.slurm
fi


}

{#$WORK/bin/slurm/Data_transform.slurm
#!/bin/bash

#SBATCH -J Transformer      					    # Name of the job
#SBATCH -o Transformer.out    				 	# Name of file that will have program output
#SBATCH -e Transformer.err      					# Name of the file that will have job errors, if any
#SBATCH -N 1                    				# Number of nodes
#SBATCH -n 20                   				# Number of cores
#SBATCH -p normal               				# Partition
#SBATCH --mail-user=andrew.fields@tamucc.edu	# Email address
#SBATCH --mail-type=end         				# Email me when the job ends
#SBATCH --time=96:00:00							# Time before the script is automatically ended

ls -d node* | xargs -P 20 -I {} sh $WORK/bin/Data_transform.sh {}
}

{#$WORK/bin/slurm/Polywoggle_all.slurm
#!/bin/bash

#SBATCH -J Polywoggle      					    # Name of the job
#SBATCH -o Polywoggle.out    				 	# Name of file that will have program output
#SBATCH -e Polywoggle.err      					# Name of the file that will have job errors, if any
#SBATCH -N 1                    				# Number of nodes
#SBATCH -n 20                   				# Number of cores
#SBATCH -p normal               				# Partition
#SBATCH --mail-user=andrew.fields@tamucc.edu	# Email address
#SBATCH --mail-type=end         				# Email me when the job ends
#SBATCH --time=96:00:00							# Time before the script is automatically ended

module load gcc/7.3.0 openblas/local/gcc7/0.3.3 R/gcc7/3.6.1 udunits/2.2.26 geos/gcc7/3.7.1 gdal/gcc7/2.3.0 proj/gcc7/5.2.0

ls -d node* | xargs -P 20 -I {} sh $WORK/bin/Data_transform.sh {}
wait
ls -d node* | xargs -P 20 -I {} Rscript $WORK/bin/MultiPolywoggle.R {} "*.txt"
}

{#$WORK/bin/slurm/Polywoggle_short.slurm
#!/bin/bash

#SBATCH -J Polywoggle      					    # Name of the job
#SBATCH -o Polywoggle.out    				 	# Name of file that will have program output
#SBATCH -e Polywoggle.err      					# Name of the file that will have job errors, if any
#SBATCH -N 1                    				# Number of nodes
#SBATCH -n 20                   				# Number of cores
#SBATCH -p normal               				# Partition
#SBATCH --mail-user=andrew.fields@tamucc.edu	# Email address
#SBATCH --mail-type=end         				# Email me when the job ends
#SBATCH --time=96:00:00							# Time before the script is automatically ended

module load gcc/7.3.0 openblas/local/gcc7/0.3.3 R/gcc7/3.6.1 udunits/2.2.26 geos/gcc7/3.7.1 gdal/gcc7/2.3.0 proj/gcc7/5.2.0

ls -d node* | xargs -P 20 -I {} Rscript $WORK/bin/MultiPolywoggle.R {} "*.txt"
}

{#$WORK/bin/MultiPolywoggle.R
#!/bin/R
#Usage: Rscript MultiPolywoggle.R <path to files> <particle.data prefix>

#Get arguements
args <- commandArgs(trailingOnly=TRUE)
PATH <- args[1]
PATTERN <- args[2]

#Libraries
suppressMessages(library(raster))
suppressMessages(library(rgdal))
suppressMessages(library(sf))
suppressMessages(library(spatialEco))

#Read shapefile
polys <- readOGR(dsn="/work/marinegenomics/afields3/Workspace/red_snapper/model/TACC/support/", layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))

#Get files list
files <- list.files(path=PATH,pattern=PATTERN)
files <- files[grep("intersect",files,invert=T)]

#Get Catch sites
dat.g <- read.table("/work/marinegenomics/afields3/Workspace/red_snapper/model/TACC/support/Caught_grid.txt", head=T)

#print(c(PATH, PATTERN, files))

for(i in files){
print(paste("processing",i))
RELEASE <- i
day <- matrix(unlist(strsplit(i,"_")))[3,1]

#Make release file
release <- read.table(RELEASE,head=F)
colnames(release) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(release$Longitude[1] > 0){release$Longitude <- release$Longitude -360}
release$Date <- rep(day,nrow(release))

# prepare coordinates, data, and proj4string
coords <- release[ , c('Longitude','Latitude')]
dat   <- release[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date")]
crs    <- CRS("+init=epsg:3160")

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= dat, proj4string = crs)

#Look for intersection
new_shape <- point.in.poly(pts, polys)

#Combine with location data
tmp.dat <- cbind(as.data.frame(pts@coords), new_shape@data)

cat.dat <- tmp.dat[tmp.dat$N %in% dat.g$N, ]

#Output the results
write.table(tmp.dat, paste(i,"intersect",sep="."), col.names=T, row.names=F, quote=F, sep="\t")

if(exists("cat.dat")>0){
if(length(list.files(path=PATH,pattern="Particles_at_Catch.tab"))==0){write.table(cat.dat, "Particles_at_Catch.tab", col.names=T, row.names=F, quote=F, sep="\t")
}else if(length(list.files(path=PATH,pattern="Particles_at_Catch.tab"))==1){write.table(cat.dat, "Particles_at_Catch.tab", col.names=F, row.names=F, quote=F, sep="\t", append=TRUE)}
}
rm(cat.dat)
}

#Notebook cleanup
}
}
}
}
}


{#$WORK/bin/Data_transform.sh
#!/bin/bash
#Usage: Data_transform.sh <directory> <Group_ID>

cd $1
sed 's:cat/::g' $1/$2 > traj_filelist
ls traj_file* | sed 's/traj_file_//g' > traj_filelist
echo "File list made"
cat traj_filelist | while read i; do 
echo $i
tail -n40000 $i | awk -v FS=" " '$3>=2246400{print $0}' | awk -v FS=" " '$8==0{print $0}' > $i.txt
done

}

GROUP=$(ls group*)
ls -d node* | xargs -P 20 -I {} sh $WORK/bin/Data_transform.sh {} $GROUP

for i in $(ls -d node.*); do
cd $i
GROUP=$(ls group*)
sh $WORK/bin/Data_transform.sh . $GROUP
done

for i in $(ls -d node.*); do
cd $i
GROUP=$(ls group*)
echo $i $GROUP
sh $WORK/bin/Data_transform.sh . $GROUP
cd ..
done

for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking the number of .txt files present in the node folders
for i in $(ls -d node.*); do
TMP=$(ls $i/*.txt | wc -l)
echo $i $TMP
done

#Checking the size of .txt files present in the node folders
for i in $(ls -d node.*); do
echo $i
TMP=$(ls $i/*.txt | wc -l)
TMP2=$(ls -l $i/*.txt | cut -f5 -d" " | awk '$1>1000000{print $0}' | wc -l)
echo $TMP $TMP2
done

ls -l node.00/*.txt | awk -v FS=" " '$5<1000000{print $0}' | head

for i in $(ls -d node.*); do
echo $i


{#Getting the ending polygon for each particle
ls -d node.* | while read i; do 
while [ $(squeue -u afields3 | grep afields | wc -l) -gt 7 ]; do sleep 60; done
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done
}


######## Scratch work ########
{#Testing MultiPolywoggle.R
tail -n30000 traj_file_Aug15_group.0000 | awk -v FS=" " '$3==2246400{print $0}' > Aug15_26_g0.txt
tail -n30000 traj_file_Aug15_group.0000 | awk -v FS=" " '$3==2332800{print $0}' > Aug15_27_g0.txt
tail -n30000 traj_file_Aug15_group.0000 | awk -v FS=" " '$3==2419200{print $0}' > Aug15_28_g0.txt
tail -n150000 traj_file_Aug15_group.0000 | awk -v FS=" " '$3==1296000{print $0}' > Aug15_15_g0.txt
time Rscript MultiPolywoggle.R . *_g0.txt
}


{#Testing the Polywoggle
module load sinteractive
sinteractive

tail -n40000 traj_file_Aug15_group.0001 | awk -v FS=" " '$3>=2246400{print $0}' | awk -v FS=" " '$8==0{print $0}' > Aug15_group.0001.txt

module load gcc/7.3.0 openblas/local/gcc7/0.3.3 R/gcc7/3.6.1 udunits/2.2.26 geos/gcc7/3.7.1 gdal/gcc7/2.3.0 proj/gcc7/5.2.0

R

#Libraries
suppressMessages(library(raster))
suppressMessages(library(rgdal))
suppressMessages(library(sf))
suppressMessages(library(spatialEco))

#Read shapefile
polys <- readOGR(dsn="/work/marinegenomics/afields3/Workspace/red_snapper/model/TACC/support", layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))

#Make release file
release <- read.table("Aug15_group.0001.txt",head=F)
colnames(release) <- c("Start", "Particle", "Time", "Longitude", "Latitude", "Depth", "Distance", "Exit_code", "Release_date")
if(release$Longitude[1] > 0){release$Longitude <- release$Longitude -360}

# prepare coordinates, data, and proj4string
coords <- release[ , c('Longitude','Latitude')]
dat   <- release[ , c("Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date")]
crs    <- CRS("+init=epsg:3160")

# make the SpatialPointsDataFrame object
pts <- SpatialPointsDataFrame(coords = coords, data= dat, proj4string = crs)

#Informing user of progress
print("Data loaded scuccessfully")
print("Comparing files")

#Look for intersection
new_shape <- point.in.poly(pts, polys)

#Combine with location data
tmp.dat <- cbind(as.data.frame(pts@coords), new_shape@data)

#Output the results
write.table(tmp.dat, "Aug15_group.0001.out", col.names=T, row.names=F, quote=F, sep="\t")

}
}
}




{#Making .txt files for the Polywoggle
#Waiting for Tom Merrick to get back to me about the R packages on HPC
cd $WORK/Workspace/red_snapper/model/TACC/2009
sbatch $WORK/bin/slurm/Data_analysis.slurm
}

{#Getting locations associated with the grids fish were caught in
cut -f3 support/Caught_grid.txt | grep -wF -f - tmp/try1.txt | less

#Works, but might need to do this through R or with an awk thing since I think only where the particle ends up is the important part
}

cd /work/marinegenomics/afields3/Workspace/red_snapper/model/TACC/2009
module load gcc/7.3.0 openblas/local/gcc7/0.3.3 R/gcc7/3.6.1 udunits/2.2.26 geos/gcc7/3.7.1 gdal/gcc7/2.3.0 proj/gcc7/5.2.0
mkdir tmp
cd tmp
cp -s ../cat/traj_file_*_group.0000 .

ls *_group.0000 | while read i; do
tail -n40000 $i | awk -v FS=" " '$3>=2246400{print $0}' | awk -v FS=" " '$8==0{print $0}' > $i.txt
done

Rscript /work/marinegenomics/afields3/bin/MultiPolywoggle.R . "*.txt"

R
dat.g <- read.table("/work/marinegenomics/afields3/Workspace/red_snapper/model/TACC/support/Caught_grid.txt", head=T)
dat.r <- read.table("traj_file_Aug15_group.0000.txt.intersect", head=T)

dat.r[dat.r$N %in% dat.g$N,]


` # Worked on 2009 samples # `
#Issue with the numeric columns trying to be characters at times
{#Brainstorming on what to do with Particles_at_Catch.tab
#HPC
cd /work/marinegenomics/afields3/Workspace/red_snapper/model/TACC/2009
scp node.00/Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis
scp ../support/Caught_grid.txt afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis

#DT
cd /home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis
#Run for each year independently
{```{R}```
#Libraries
library('dplyr')
library('ymse')
library('scales')
library('DescTools')

#Parameter alterations
options("scipen"=100, "digits"=4)

#Read in Particle data
dat <- read.table("Particles_at_Catch_2011.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
#names(dat) <- c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N")
dat <- dat[!(is.na(dat$N)),]
dat$Start <- as.character(dat$Start)
head(dat)

#Read Catch locations
catch <- read.table("Caught_grid.txt", head=T)
catch <- catch[!(is.na(catch$N)),]
catch <- catch[order(catch$N),]
catch <- catch[!duplicated(catch[,'N']),]

#Get all the grid in the data
#grid.loc <- unique(c(dat$Start,dat$N))
#head(grid.loc)
#length(grid.loc)
#conn.sq <- data.frame(matrix(ncol=length(grid.loc), nrow=length(grid.loc)))
#colnames(conn.sq) <- rownames(conn.sq) <- grid.loc

#Make data frames for data
direct.link <- data.frame(matrix(ncol=4))
indirect.link <- data.frame()

#For each end location, divide starts
for(i in unique(dat$N)){
tmp.dat <- dat[dat$N==i,]

#Establish which links between catch grids
links <- table(tmp.dat[which(tmp.dat$Start %in% catch$N),"Start"])
if(length(links)>0){
tmp.links <- data.frame(matrix(ncol=4, nrow=length(links)))
tmp.links[,1] <- rep(i,length(links))
tmp.links[,2] <- as.numeric(names(links))
tmp.links[,3] <- links
tmp.links[,4] <- links/330000		#11 release dates * 10,000 particles per date * 3 days analyzing over
direct.link <- rbind(direct.link,tmp.links)}

# Establish which links between catch grids
tmp.inlinks <- tmp.dat %>% count(N,count = Start)
tmp.inlinks$Percent <- tmp.inlinks$n/330000
indirect.link <- rbind(indirect.link,as.data.frame(tmp.inlinks))
}
direct.link <- direct.link[-1,]
names(direct.link) <- names(indirect.link) <- c("End", "Start", "count", "Percent")

tmp.hold <- direct.link
head(direct.link)
dim(direct.link)

#Refresh
direct.link <- tmp.hold
direct.link$End <- as.numeric(direct.link$End)

# Analyzing the indirect links
# Setting up the table for the data
direct.link$indirect <- rep(0,nrow(direct.link))
direct.link$indirect <- as.numeric(direct.link$indirect)
#Getting the starting locations
tmp.tab <- table(indirect.link$Start)

# Loop through all the starting locations
for(i in names(tmp.tab)[tmp.tab>1]){
# Get data for a given start site
tmp.dat <- indirect.link[indirect.link$Start==i,]
# Adding indirect interactions to the direct table
for(j in tmp.dat$End){
# Last lines are skipped
if(which(tmp.dat$End==j)==length(tmp.dat$End)){next}
# Doing a pairwise addition
for(k in tmp.dat$End[(which(tmp.dat$End==j)+1):nrow(tmp.dat)]){
#Indirect effects are symetrical and therefore have to be added in both directions
# If the combination exists, find it and add the indirect interaction. If it does not exist then add it
if(nrow(direct.link[direct.link$Start==j & direct.link$End==k,])>0){
direct.link$indirect[direct.link$Start==j & direct.link$End==k] <- as.numeric(direct.link$indirect[direct.link$Start==j & direct.link$End==k]) + tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k]
} else if(nrow(direct.link[direct.link$Start==j & direct.link$End==k,])==0){
tmp.line <- c(j,k,0,0,tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k])
direct.link <- rbind(direct.link, tmp.line)
}

if(nrow(direct.link[direct.link$Start==k & direct.link$End==j,])>0){
direct.link$indirect[direct.link$Start==k & direct.link$End==j] <- as.numeric(direct.link$indirect[direct.link$Start==k & direct.link$End==j]) + tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k]
} else if(nrow(direct.link[direct.link$Start==k & direct.link$End==j,])==0){
tmp.line <- c(k,j,0,0,tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k])
direct.link <- rbind(direct.link, tmp.line)
}
}}}

for(i in c("Percent", "indirect")){direct.link[,i] <- as.numeric(as.matrix((direct.link[,i])))}
direct.link$Total <- apply(direct.link[,c("Percent", "indirect")],1,sum)

#Plotting it
tiff("2011_Similarities.tif", res=200, height=3000, width=1500)
par(mfrow=c(2,1))
hist(as.numeric(direct.link$Percent), breaks=100, col=alpha("red4",0.5), xlab="Site interaction", main=NULL)
hist(as.numeric(direct.link$Total), breaks=100, col=alpha("mediumblue", 0.5), add=T)
legend(0.14, 70000,legend=c("Direct","Direct+Indirect", "Both"), col=c(alpha("red4",0.5), alpha("mediumblue",0.5), MixColor(alpha("red4",0.5),alpha("mediumblue",0.5),0.5)), pch=15)

hist(as.numeric(direct.link$Percent), breaks=100, col=alpha("red4",0.5), ylim=c(0,25), xlab="Site interaction", main=NULL)
hist(as.numeric(direct.link$Total), breaks=100, col=alpha("mediumblue", 0.5), ylim=c(0,25), add=T)
legend(0.14, 20,legend=c("Direct","Direct+Indirect", "Both"), col=c(alpha("red4",0.5), alpha("mediumblue",0.5), MixColor(alpha("red4",0.5),alpha("mediumblue",0.5),0.5)), pch=15)
dev.off()

write.table(direct.link, "2011_link.txt", quote=F, row.names=F, col.names=T)

}
}
}
}
}
}
}
}

#Combining Different years
{```{R}```
#Parameter alterations
options("scipen"=100, "digits"=4)

#Read Catch locations
catch <- read.table("Caught_grid.txt", head=T)
catch <- catch[!(is.na(catch$N)),]
catch <- catch[order(catch$N),]
catch <- catch[!duplicated(catch[,'N']),]

#Get the years
dat.2009 <- read.table("2009_link.txt", head=T)
dat.2010 <- read.table("2010_link.txt", head=T)
dat.2012 <- read.table("2012_link.txt", head=T)

#Put years into the matrix
for(j in c("2009","2010","2012")){
#Making a df of all catch sites
#Rows are the Start locations; Columns are the End locations
print(paste("starting", j))
k <- paste("dat",j,sep=".")
tmp <- data.frame(matrix(nrow=nrow(catch), ncol=nrow(catch)))
names(tmp) <- rownames(tmp) <- catch$N
tmp[is.na(tmp)] <- 0

for(i in 1:nrow(get(k))){
tmp[rownames(tmp)==get(k)[i,"Start"],names(tmp)==get(k)[i,"End"]] <- get(k)[i,"Total"]
}
assign(paste("df",j,sep="."),tmp)
}

#Averaging all the years
df.all <- (df.2009 + df.2010 + df.2012)/3

#Rows are the Start locations; Columns are the End locations
write.table(df.all, "", col.names=T, row.names=T, quote=F)
}

}}}}}}}}}}}}}}}}}}}Clean-up

#Converting the model output to usable tables for analysis
#Working on 2009
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2009
mkdir 2009
mv 2009_model.tar May15_2009.tar 2009/
cd 2009
tar -xf 2009_model.tar
tar -xf May15_2009.tar

#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 19); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#53340
echo "53340/20" | bc
#2667
echo "2668*20" | bc
#58680

split -l 2934 -d filelist group

seq -w 0 19 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
ERR=$(grep "cannot open" ../Transformer_${i}.err | wc -l)
if [ $ERR -gt 0 ] ; then echo "$i Failed: $ERR errors, please check and rerun";
elif (( $FILES > $TXT )) ; then echo -e "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo -e "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo -e "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Repreparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Making the final file
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Remaking May15
#Getting the Workspace ready for analysis
{```{bash}```
#Making the new node folders
for i in $(seq -w 30 31); do mkdir node.$i; done
ls cat/*May15* > filelist
wc -l filelist

#split on HPC cannot split by number of groups
split -l 2667 -d filelist group
mv group00 group30
mv group01 group31

seq -w 30 31 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done

#Preparing files for Pollywoggle
for i in $(ls -d node.* | tail -n2); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | tail -n2 | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | tail -n2 | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
fi
sleep 5
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | tail -n2 | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
#echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Making the final file
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Looking at the number of records per release date
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
TMP=$(grep -w $j Particles_at_Catch.tab | wc -l)
echo $j $TMP
done

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2009.tab

#Cleaning up HPC
cd ..
tar -zcf 2009_model.tgz 2009/cat 2009/Particles_at_Catch.tab

sbatch --nodelist=hpcc42 --export=YEAR=2009 -o tarball_2009.out -e tarball_2009.err $WORK/bin/slurm/tarball.slurm
#Checking tgz
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2009/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2009_model.tgz | grep _$j_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2009.log
done

#Erasing files
rm 2009/node.* 2009/*.out 2009/.err 2009/May15_2009.tar
rm 2009/cat 2009/filelist

}}}}}}}}}}} #Cleanup

#Working on 2010
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC/2010
#Preparing files for Pollywoggle		#Started 3:30pm 9/23/20
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file		#Started 2:15pm 9/24/20
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking and concatinating Pollywoggle files
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2010.tab

#Cleaning up HPC
cd ..
tar -zcf 2010_model.tgz 2010/cat 2010/Particles_at_Catch.tab

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2010/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2010_model.tgz | grep "_${j}_" | wc -l)
echo $j $CAT $TAR 2>&1 | tee -a tarball_2010.log
done

less tarball_2010.log

#Erasing files
rm -r 2010/node.* 2010/*.out 2010/*.err 2010/test.dir
rm -r 2010/cat 2010/May15_2010.tar 2010/filelist

}}}}}}}}}}} #Cleanup

#Working on 2011
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC/2011
#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully		#Checked and are good
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file 				#Started 9/25/20 4:00pm
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking and concatinating Pollywoggle files
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2011.tab

#Cleaning up HPC
cd ..
tar -zcf 2011_model.tgz 2011/cat 2011/Particles_at_Catch.tab

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2011/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2010_model.tgz | grep "_${j}_" | wc -l)
echo $j $CAT $TAR 2>&1 | tee -a tarball_2011.log
done

less tarball_2011.log

rm -r 2011/node.* 2011/*.out 2011/*.err
rm -r 2011/cat 2011/May15_2011.tar 2011/filelist

}}}}}}}}}}} #Cleanup

#Working on 2012
{```{bash}```
{#Getting the Workspace ready for analysis
cd $WORK/Workspace/red_snapper/model/TACC/2012
#preparing to run the modeling on all the 2009 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#58674
echo "58674/24" | bc
#2444
echo "2445*24" | bc

split -l 2445 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle				#Started 2:30pm 9/24/20
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully	#Checked and are good
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file		 		#Started 9/25/20 4:00pm
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking and concatinating Pollywoggle files
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2012.tab

#Cleaning up HPC
cd ..
tar -zcf 2012_model.tar 2012/cat 2012/Particles_at_Catch.tab

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2012/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2010_model.tgz | grep "_${j}_" | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2012.log
done

less tarball_2012.log

#Erasing files
rm -r 2012/node.* 2012/*.out 2012/*.err
rm -r 2012/cat 2012/filelist


}}}}}}}}}}} #Cleanup

#Working on 2013
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2013
mkdir 2013
mv 2013_model.tar May15_2013.tar 2013/
cd 2013
tar -xf 2013_model.tar
tar -xf May15_2013.tar

#Getting the Workspace ready for analysis		#Started 9/25/20 4:15pm
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#58674
echo "58674/24" | bc
#2444
echo "2445*24" | bc

split -l 2445 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Power outage ended the loop early
#Making the Particle_Catch.tab file for nodes 17-23
ls -d node.* | tail -n+18 |  while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2013.tab

#Cleaning up HPC
cd ..
tar -zcf 2013_model.tgz 2013/cat 2013/Particles_at_Catch.tab
#tar: 2013/cat/traj_file_Aug1_group.0104: file changed as we read it

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2013/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2013_model.tgz | grep _$j_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2013.log
done

#Erasing files
rm 2013/node.* 2013/*.out 2013/.err 2013/May15_2013.tar
rm 2013/cat 2013/filelist

}}}}}}}}}}} #Cleanup

#Working on 2014
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2014
mkdir 2014
mv 2014_model.tar May15_2014.tar 2014/
cd 2014
tar -xf 2014_model.tar
tar -xf May15_2014.tar

#Getting the Workspace ready for analysis		#Started 9/25/20 4:15pm
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#58674
echo "58674/24" | bc
#2444
echo "2445*24" | bc

split -l 2445 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Power outage ended the loop early
#Preparing files for Pollywoggle
for i in $(ls -d node.* | tail -n+16); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Picking up where it left off
ls -d node.* | tail -n +24 | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 7 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#7 of the Polywoggles are going on 3 days when they should take 12-15 hrs.

#Nodes 15, 16, 17, 19, 20, 21 & 22 are not done and have not changed since 10am on friday
#Running files which did not work or did not start
echo -e "node.15\nnode.16\nnode.17\nnode.19\nnode.20\nnode.21\nnode.22\nnode.23" | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 7 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Covering incomplete nodes
echo -e "node.15\nnode.20" | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 7 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
done

#Making the final file
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2014.tab

#Cleaning up HPC
cd ..
tar -zcf 2014_model.tgz 2014/cat 2014/Particles_at_Catch.tab

#Checking tgz
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2014/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2014_model.tgz | grep "_${j}_" | wc -l)
echo $j $CAT $TAR 2>&1 | tee -a tarball_2014.log
done

#Erasing files
rm -r *out *.err 2014_model.tar May15_2014.tar node.* cat filelist


#Remaking Aug15
#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense

for i in $(seq -w 0 1); do mkdir node.$i; done
ls cat/*Aug15* > filelist

#split on HPC cannot split by number of groups
split -l 2667 -d filelist group -a 1

seq -w 0 1 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
fi
sleep 5
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
#echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Looking at the number of records per release date
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
TMP=$(grep -w $j Particles_at_Catch.tab | wc -l)
echo $j $TMP
done

#making a copy
cp Particles_at_Catch.tab Particles_at_Catch.tab_old

#Removing Aug15 from the list
grep -v Aug15 Particles_at_Catch.tab > tmp.file
mv tmp.file Particles_at_Catch.tab

#Adding Aug15 to the Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab >> Particles_at_Catch.tab

#Looking at the number of records per release date
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
TMP=$(grep -w $j Particles_at_Catch.tab | wc -l)
echo $j $TMP
done

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2014.tab

#Cleaning up HPC
cd ..
tar -zcf 2014_model.tgz 2014/cat 2014/Particles_at_Catch.tab

YEAR=2014; sbatch --nodelist=hpcc44 --export=YEAR=$YEAR -o tarball_${YEAR}.out -e tarball_${YEAR}.err $WORK/bin/slurm/tarball.slurm

#Checking tgz
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2014/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2014_model.tgz | grep "_${j}_" | wc -l)
echo $j $CAT $TAR 2>&1 | tee -a tarball_2014.log
done

#Erasing files
rm -r *out *.err 2014_model.tar May15_2014.tar node.* cat filelist


}}}}}}}}}}} #Cleanup

#Working on 2015
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2015
mkdir 2015
mv 2015.tgz 2015/
cd 2015
tar -zxf 2015.tgz
mv 2015 cat

#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2015 files
#With one file for each release, just going to make one file node per release date
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
mkdir $j
cd $j
cp -s ../cat/${j}_* .
cd ..
done

#Preparing files for Pollywoggle
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform_alt.slurm
done

#Check if the .txt files were made successfully
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read i; do
ls -lh $i/*txt
done

#Making the Particle_Catch.tab file
while [ $(squeue | grep 138878 | wc -l) -gt 0 ]; do sleep 60; done
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
ls -d */ | while read i; do
ls -lh $i/Particles_at_Catch.tab
done

ls -d */ | grep -v cat | while read i; do
wc -l $i/Particles_at_Catch.tab
done

rm Particles_at_Catch.tab
ls -d */ | grep -v cat | while read i; do
cat $i/Particles_at_Catch.tab >> Particles_at_Catch.tab
done

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2015.tab

#Cleaning up HPC
cd ..
tar -zcf 2015_model.tgz 2015/cat 2015/Particles_at_Catch.tab

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2015/cat/${j}_* | wc -l)
TAR=$(tar -tvf 2015_model.tgz | grep _$j_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2015.log
done

#Erasing files
cd 2015
rm -r *.out *.err Aug1_traj_file_all.txt 2015.tgz *1 *15
rm -r cat

}}}}}}}}}}} #Cleanup

#Working on 2016
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2014
mkdir 2016
mv 2016_model.tar 2016/
cd 2016
tar -xf 2016_model.tar

#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2016 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#58674
echo "58674/24" | bc
#2444
echo "2445*24" | bc

split -l 2445 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Power outage ended the loop early
#Preparing files for Pollywoggle
for i in $(ls -d node.* | tail -n+16); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
if (( $FILES > $TXT )) ; then echo "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 7 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2016.tab

#Cleaning up HPC
cd ..
tar -zcf 2016_model.tgz 2016/cat 2016/Particles_at_Catch.tab
#tar: 2016/cat: file changed as we read it

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2016/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2016_model.tgz | grep ${j}_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2016.log
done

#Erasing files
rm -r 2016/node.* 2016/*.out 2016/.err 2016/cat 2016/filelist

}}}}}}}}}}} #Cleanup

#Working on 2017
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2017
mkdir 2017
mv 2017_model.tar 2017/
cd 2017
tar -xf 2017_model.tar
tar -xf Sep1_2017.tar

#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls cat/*_${j}_* | wc -l)
echo $j $CAT  2>&1
done
{#Results
Aug1 5334
Aug15 5334
Jul1 5334
Jul15 5334
Jun1 5334
Jun15 5334
May1 5334
ls: cannot access cat/*_May15_*: No such file or directory
May15 0
ls: cannot access cat/*_Oct1_*: No such file or directory
Oct1 0
Sep1 5334
ls: cannot access cat/*_Sep15_*: No such file or directory
Sep15 0
}

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#37338
echo "37338/24" | bc
#1555
echo "1556*24" | bc
#37344

split -l 1556 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
sbatch --nodelist=hpcc44 --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
sleep 5
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
ERR=$(grep "cannot open" ../Transformer_${i}.err | wc -l)
if [ $ERR -gt 0 ] ; then echo "$i Failed: $ERR errors, please check and rerun";
elif (( $FILES > $TXT )) ; then echo -e "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo -e "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo -e "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
fi
sleep 5
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Nodes 21 and 22 did not work
echo -e "node.21\nnode.22" | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
fi
sleep 5
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2017.tab

#Cleaning up HPC
cd ..
tar -zcf 2017_model.tgz 2017/cat 2017/Particles_at_Catch.tab
#Could not tarball; files would not read
cd 2017
rm cat/*
tar -xf 2017_model.tar 
tar -xf Sep1_2017.tar

cd ..
tar -zcf 2017_model.tgz 2017/cat 2017/Particles_at_Catch.tab

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2017/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2017_model.tgz | grep _$j_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2017.log
done

#Erasing files
rm 2017/node.* 2017/*.out 2017/.err 2017/May15_2017.tar
rm 2017/cat 2017/filelist

}}}}}}}}}}} #Cleanup

#Working on 2018
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2018
mkdir 2018
mv 2018_model.tar May15_2018.tar 2018/
cd 2018
tar -xf --overwrite 2018_model.tar
tar -xf --overwrite May15_2018.tar

#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#58674
echo "58674/24" | bc
#2444
echo "2445*24" | bc
#58680

split -l 2445 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

for i in $(ls -d node.*| tail -n2 | head -n1); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --nodelist=hpcc41 --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | tail -n1 | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
ERR=$(grep "cannot open" ../Transformer_${i}.err | wc -l)
if [ $ERR -gt 0 ] ; then echo "$i Failed: $ERR errors, please check and rerun";
elif (( $FILES > $TXT )) ; then echo -e "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo -e "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo -e "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | tail -n1 | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Repreparing files for Pollywoggle
for i in $(ls -d node.*| tail -n2); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Making the Particle_Catch.tab file
ls -d node.* | tail -n2 | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

#Making the final file
ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2018.tab

#Cleaning up HPC
cd ..
tar -zcf 2018_model.tgz 2018/cat 2018/Particles_at_Catch.tab

YEAR=2018; sbatch --nodelist=hpcc44 --export=YEAR=$YEAR -o tarball_${YEAR}.out -e tarball_${YEAR}.err $WORK/bin/slurm/tarball.slurm

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2018/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2018_model.tgz | grep _$j_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2018.log
done

#Erasing files
rm 2018/node.* 2018/*.out 2018/.err 2018/May15_2018.tar
rm 2018/cat 2018/filelist

}}}}}}}}}}} #Cleanup

#Working on 2019
{```{bash}```
cd $WORK/Workspace/red_snapper/model/TACC
#Unzipping 2019
mkdir 2019
mv 2019_model.tar May15_2019.tar 2019/
cd 2019
tar -xf 2019_model.tar
tar -xf May15_2019.tar

#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense
for i in $(seq -w 0 23); do mkdir node.$i; done
ls cat/*group* > filelist

#split on HPC cannot split by number of groups
#Manual split of files
wc -l filelist
#58674
echo "58674/24" | bc
#2444
echo "2445*24" | bc
#58680

split -l 2445 -d filelist group

seq -w 0 23 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.*); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 9 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Check if the .txt files were made successfully
ls -d node.* | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
ERR=$(grep "cannot open" ../Transformer_${i}.err | wc -l)
if [ $ERR -gt 0 ] ; then echo "$i Failed: $ERR errors, please check and rerun";
elif (( $FILES > $TXT )) ; then echo -e "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo -e "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo -e "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | while read i; do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm
done

#Checking to see how the outputs look
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
done

#Making the Particle_Catch.tab file
echo -e "node.10\nnode.11\nnode.17\nnode.18\nnode.19\nnode.20\nnode.20\nnode.22\nnode.23" | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
fi
sleep 5
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2019.tab

#Cleaning up HPC
cd ..
tar -zcf 2019_model.tgz 2019/cat 2019/Particles_at_Catch.tab
#tar cannot find all the files

#Erasing files
rm 2019/node.* 2019/*.out 2019/.err 2019/May15_2019.tar
rm 2019/cat 2019/filelist


#Remaking Sep15
#Getting the Workspace ready for analysis
{```{bash}```
#preparing to run the modeling on all the 2013 files
#With only 8 projects allowed at a time, doing multiples of 8 makes more sense

for i in $(seq -w 30 31); do mkdir node.$i; done
ls cat/*Sep15* > filelist

#split on HPC cannot split by number of groups
split -l 2667 -d filelist group
mv group00 group30
mv group01 group31

seq -w 30 31 | while read i; do
echo "Processing node.${i}"
mv group$i node.$i
cd node.$i
cat group$i | while read j; do
cp -s ../$j .
done
cd ..
done
}

#Preparing files for Pollywoggle
for i in $(ls -d node.* | tail -n2); do
while [ $(squeue | grep afields | awk '$5=="R" {print $0} $5=="PD" {print $0}' | wc -l) -ge 8 ]; do sleep 60; done;
sbatch --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm
done

#Alt approach to preparing files for Pollywoggle
ls -d node.* | tail -n2 | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Transformer_$i.out -e Transformer_$i.err $WORK/bin/slurm/Data_transform.slurm;
fi
sleep 5
done

#Check if the .txt files were made successfully
ls -d node.* | tail -n2 | while read i; do
cd $i
FILES=$(wc -l group* | cut -f1 -d " ")
TXT=$(ls *.txt | wc -l)
ERR=$(grep "cannot open" ../Transformer_${i}.err | wc -l)
if [ $ERR -gt 0 ] ; then echo "$i Failed: $ERR errors, please check and rerun";
elif (( $FILES > $TXT )) ; then echo -e "$i Failed: Too few .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES < $TXT )) ; then echo -e "$i Failed: Too many .txt files\nFiles expected:$FILES\nTxt files:$TXT";
elif (( $FILES == $TXT )) ; then echo -e "$i is good"; fi
cd ..
done

#Making the Particle_Catch.tab file
ls -d node.* | tail -n2 | while read i; do
NODE41=$(squeue | grep hpcc41 | wc -l)
NODE42=$(squeue | grep hpcc42 | wc -l)
NODE44=$(squeue | grep hpcc44 | wc -l)
echo $NODE41 $NODE42 $NODE44
if [[ $NODE41 -lt $NODE42 && $NODE41 -lt $NODE44 || $NODE41 -eq $NODE42 && $NODE41 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc41 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE42 -lt $NODE41 && $NODE42 -lt $NODE44 || $NODE42 -eq $NODE44 ]] ; then sbatch --nodelist=hpcc42 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
elif [[ $NODE44 -lt $NODE41 && $NODE44 -lt $NODE42 ]] ; then sbatch --nodelist=hpcc44 --export=NODE=$i -o Polywoggle_$i.out -e Polywoggle_$i.err $WORK/bin/slurm/Polywoggle_short.slurm;
fi
sleep 5
done

#Checking to see how the outputs look
rm incomplete_node
ls -d node.* | while read i; do
END=$(tail -n1 $i/group* | sed 's:cat/traj_file_::g')
COUNT=$(grep $END Poly*${i}.out | wc -l)
echo $i $END $COUNT
if [ $COUNT -eq 2 ] ; then echo "$i completed successfully"; 
elif [ $COUNT -eq 1 ] ; then echo -e "$i started, but did not complete\n written to incomplete_node"; echo $i >> incomplete_node;
elif [ $COUNT -eq 0 ] ; then echo -e "$i did not start\n written to incomplete_node";  echo $i >> incomplete_node;
fi
done

ls -lh node.*/Particles_at_Catch.tab
cat node.*/Particles_at_Catch.tab > Particles_at_Catch.tab

ls -d node.* | while read i; do 
if [[ $i == node.00 ]] ; then cat $i/P* > Particles_at_Catch.tab;
else tail -n+2 $i/P* >> Particles_at_Catch.tab; fi
done

grep -v Sep15 Particles_at_Catch.tab > tmp.file; mv tmp.file Particles_at_Catch.tab
cat node.3*/Particles_at_Catch.tab >> Particles_at_Catch.tab

#Moving to deepthought
scp Particles_at_Catch.tab afields@deepthought.ad.tamucc.edu:/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis/Particles_at_Catch_2019.tab

#Cleaning up HPC
cd ..
tar -zcf 2019_model.tgz 2019/cat 2019/Particles_at_Catch.tab
#tar cannot find all the files

YEAR=2019; sbatch --nodelist=hpcc42 --export=YEAR=$YEAR -o tarball_${YEAR}.out -e tarball_${YEAR}.err $WORK/bin/slurm/tarball.slurm
while [ $(squeue -u afields3 | grep 140418 | grep PD | wc -l) -gt 0 ] ; do sleep 60; done; tail -F tarball_2019.err

#Checking tgz 
echo -e Aug1"\n"Aug15"\n"Jul1"\n"Jul15"\n"Jun1"\n"Jun15"\n"May1"\n"May15"\n"Oct1"\n"Sep1"\n"Sep15 | while read j; do
CAT=$(ls 2016/cat/*_${j}_* | wc -l)
TAR=$(tar -tvf 2016_model.tgz | grep ${j}_ | wc -l)
echo $j $CAT $TAR  2>&1 | tee -a tarball_2016.log
done

#Erasing files
rm 2019/node.* 2019/*.out 2019/.err 2019/May15_2019.tar
rm 2019/cat 2019/filelist
}

#Making tarballs of the files
{```{bash}```
for YEAR in $(echo -e "2009\n2014"); do
echo $YEAR
sbatch --nodelist=hpcc41 --export=YEAR=$YEAR -o tarball_${YEAR}.out -e tarball_${YEAR}.err $WORK/bin/slurm/tarball.slurm
sleep 60
JOB=$(squeue -u afields3 | grep hpcc41 | cut -f1)
while [ $(squeue -u afields3 | grep $JOB | wc -l) -eq 1 ]; do sleep 60; done
mv .tgz $YEAR_model.tgz
done
}

}}}}}}}}}}} #Cleanup

#Looking at the Particle files to make sure they do not have line errors
`# DT #`
{```{bash}```
ls Particles_at_Catch_*.tab | while read i; do echo $i; TMP=$(awk 'NF!=11{print NF}' $i | sort -n | uniq -c); echo $TMP; done
#2014 and 2019 are bad

#Looking at the lines with length errors
awk -v FS="\t" 'NF!=11{x=$0; getline; printf x "\n" $0 "\n"}' Particles_at_Catch_2014.tab
#Aug15 issues

awk -v FS="\t" 'NF!=11{x=$0; getline; printf x "\n" $0 "\n"}' Particles_at_Catch_2019.tab
#Sep15 issues
}

#Making the probability file linking the sites for analysis
{
#DT
cd /home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/analysis

#Run for each year independently
{```{R}```
#Libraries
library('dplyr')
library('ymse')
library('scales')
library('DescTools')

#Parameter alterations
options("scipen"=100, "digits"=4)

#Read in Particle data
rm(list=ls())
dat <- read.table("2009_Particles_at_Catch.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"), fill=T)
#names(dat) <- c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N")
dat <- dat[!(is.na(dat$N)),]
dat$Start <- as.character(dat$Start)
head(dat)

#Read Catch locations
catch <- read.table("Caught_grid.txt", head=T)
catch <- catch[!(is.na(catch$N)),]
catch <- catch[order(catch$N),]
catch <- catch[!duplicated(catch[,'N']),]

#Get all the grid in the data
#grid.loc <- unique(c(dat$Start,dat$N))
#head(grid.loc)
#length(grid.loc)
#conn.sq <- data.frame(matrix(ncol=length(grid.loc), nrow=length(grid.loc)))
#colnames(conn.sq) <- rownames(conn.sq) <- grid.loc

#Make data frames for data
direct.link <- data.frame(matrix(ncol=4))
indirect.link <- data.frame()

#For each end location, divide starts
for(i in unique(dat$N)){
tmp.dat <- dat[dat$N==i,]

#Establish which links between catch grids
links <- table(tmp.dat[which(tmp.dat$Start %in% catch$N),"Start"])
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
if(length(links)>0){
tmp.links <- data.frame(matrix(ncol=4, nrow=length(links)))
tmp.links[,1] <- rep(i,length(links))
tmp.links[,2] <- as.numeric(names(links))
tmp.links[,3] <- links
tmp.links[,4] <- links/(30000*DATES)		# of release dates * 10,000 particles per date * 3 days analyzing over
direct.link <- rbind(direct.link,tmp.links)}

# Establish which links between catch grids
tmp.inlinks <- tmp.dat %>% count(N,count = Start)
tmp.inlinks$Percent <- tmp.inlinks$n/(30000*DATES)
indirect.link <- rbind(indirect.link,as.data.frame(tmp.inlinks))
}
direct.link <- direct.link[-1,]
names(direct.link) <- names(indirect.link) <- c("End", "Start", "count", "Percent")

tmp.hold <- direct.link
head(direct.link)
dim(direct.link)
{`#Results` original data
2009 direct -> 913x4
2010 direct -> 953x4
2011 direct -> 815x4
2012 direct -> 895x4
2013 direct -> 1031x4
2014 direct -> 1086x4
2015 direct -> 1120x4
2016 direct -> 969x4
2017 direct -> 926x4
2018 direct -> 1081x4
2019 direct -> 1166x4

#Results with imaginary points
2009 direct -> 968   4
}

#Refresh
direct.link <- tmp.hold
direct.link$End <- as.numeric(direct.link$End)

# Analyzing the indirect links
# Setting up the table for the data
direct.link$indirect <- rep(0,nrow(direct.link))
direct.link$indirect <- as.numeric(direct.link$indirect)
#Getting the starting locations
tmp.tab <- table(indirect.link$Start)

# Loop through all the starting locations
for(i in names(tmp.tab)[tmp.tab>1]){
# Get data for a given start site
tmp.dat <- indirect.link[indirect.link$Start==i,]
# Adding indirect interactions to the direct table
for(j in tmp.dat$End){
# Last lines are skipped
if(which(tmp.dat$End==j)==length(tmp.dat$End)){next}
# Doing a pairwise addition
for(k in tmp.dat$End[(which(tmp.dat$End==j)+1):nrow(tmp.dat)]){
#Indirect effects are symetrical and therefore have to be added in both directions
# If the combination exists, find it and add the indirect interaction. If it does not exist then add it
if(nrow(direct.link[direct.link$Start==j & direct.link$End==k,])>0){
direct.link$indirect[direct.link$Start==j & direct.link$End==k] <- as.numeric(direct.link$indirect[direct.link$Start==j & direct.link$End==k]) + tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k]
} else if(nrow(direct.link[direct.link$Start==j & direct.link$End==k,])==0){
tmp.line <- c(j,k,0,0,tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k])
direct.link <- rbind(direct.link, tmp.line)
}

if(nrow(direct.link[direct.link$Start==k & direct.link$End==j,])>0){
direct.link$indirect[direct.link$Start==k & direct.link$End==j] <- as.numeric(direct.link$indirect[direct.link$Start==k & direct.link$End==j]) + tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k]
} else if(nrow(direct.link[direct.link$Start==k & direct.link$End==j,])==0){
tmp.line <- c(k,j,0,0,tmp.dat$Percent[tmp.dat$End==j]*tmp.dat$Percent[tmp.dat$End==k])
direct.link <- rbind(direct.link, tmp.line)
}
}}}

for(i in c("Percent", "indirect")){direct.link[,i] <- as.numeric(as.matrix((direct.link[,i])))}
direct.link$Total <- apply(direct.link[,c("Percent", "indirect")],1,sum)

#Plotting it
par(mfrow=c(2,1))
hist(as.numeric(direct.link$Percent), breaks=100, col=alpha("red4",0.5), xlab="Site interaction", main=NULL)
hist(as.numeric(direct.link$Total), breaks=100, col=alpha("mediumblue", 0.5), add=T)
legend(0.14, 70000,legend=c("Direct","Direct+Indirect", "Both"), col=c(alpha("red4",0.5), alpha("mediumblue",0.5), MixColor(alpha("red4",0.5),alpha("mediumblue",0.5),0.5)), pch=15)

hist(as.numeric(direct.link$Percent), breaks=100, col=alpha("red4",0.5), ylim=c(0,25), xlab="Site interaction", main=NULL)
hist(as.numeric(direct.link$Total), breaks=100, col=alpha("mediumblue", 0.5), ylim=c(0,25), add=T)
legend(0.14, 20,legend=c("Direct","Direct+Indirect", "Both"), col=c(alpha("red4",0.5), alpha("mediumblue",0.5), MixColor(alpha("red4",0.5),alpha("mediumblue",0.5),0.5)), pch=15)

tiff("2019_Similarities.tif", res=200, height=3000, width=1500)
par(mfrow=c(2,1))
hist(as.numeric(direct.link$Percent), breaks=100, col=alpha("red4",0.5), xlab="Site interaction", main=NULL)
hist(as.numeric(direct.link$Total), breaks=100, col=alpha("mediumblue", 0.5), add=T)
legend(0.14, 70000,legend=c("Direct","Direct+Indirect", "Both"), col=c(alpha("red4",0.5), alpha("mediumblue",0.5), MixColor(alpha("red4",0.5),alpha("mediumblue",0.5),0.5)), pch=15)

hist(as.numeric(direct.link$Percent), breaks=100, col=alpha("red4",0.5), ylim=c(0,25), xlab="Site interaction", main=NULL)
hist(as.numeric(direct.link$Total), breaks=100, col=alpha("mediumblue", 0.5), ylim=c(0,25), add=T)
legend(0.14, 20,legend=c("Direct","Direct+Indirect", "Both"), col=c(alpha("red4",0.5), alpha("mediumblue",0.5), MixColor(alpha("red4",0.5),alpha("mediumblue",0.5),0.5)), pch=15)
dev.off()

write.table(direct.link, "2019_link.txt", quote=F, row.names=F, col.names=T)

}}}}}}}}}}} #Cleanup

#Combining Different years
{```{R}```
#Parameter alterations
options("scipen"=100, "digits"=4)

#Read Catch locations
catch <- read.table("Caught_grid.txt", head=T)
catch <- catch[!(is.na(catch$N)),]
catch <- catch[order(catch$N),]
catch <- catch[!duplicated(catch[,'N']),]

#Get the years
dat.2009 <- read.table("2009_link.txt", head=T)
dat.2010 <- read.table("2010_link.txt", head=T)
dat.2011 <- read.table("2011_link.txt", head=T)
dat.2012 <- read.table("2012_link.txt", head=T)
dat.2013 <- read.table("2013_link.txt", head=T)
dat.2014 <- read.table("2014_link.txt", head=T)
dat.2015 <- read.table("2015_link.txt", head=T)
dat.2016 <- read.table("2016_link.txt", head=T)
dat.2017 <- read.table("2017_link.txt", head=T)
dat.2018 <- read.table("2018_link.txt", head=T)
dat.2019 <- read.table("2019_link.txt", head=T)

#Put years into the matrix
for(j in seq(2009,2019)){
#Making a df of all catch sites
#Rows are the Start locations; Columns are the End locations
print(paste("starting", j))
k <- paste("dat",j,sep=".")
tmp <- data.frame(matrix(nrow=nrow(catch), ncol=nrow(catch)))
names(tmp) <- rownames(tmp) <- catch$N
tmp[is.na(tmp)] <- 0

for(i in 1:nrow(get(k))){
tmp[rownames(tmp)==get(k)[i,"Start"],names(tmp)==get(k)[i,"End"]] <- get(k)[i,"Total"]
}
assign(paste("df",j,sep="."),tmp)
}

#Averaging all the years
df.all <- (df.2009 + df.2010 + df.2012 + df.2013 + df.2014 + df.2015 + df.2016 + df.2017 + df.2018 + df.2019)/10

#Rows are the Start locations; Columns are the End locations
write.table(df.all, "All_link.txt", col.names=T, row.names=T, quote=F)
}
}

}}}}}}}}}}} #Cleanup


#Loooking at how many release times I have per year
{```{R}```
rm(list=ls())
dat <- read.table("Particles_at_Catch_2009.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
rm(list=ls())
dat <- read.table("Particles_at_Catch_2010.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
rm(list=ls())
dat <- read.table("Particles_at_Catch_2011.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
rm(list=ls())
dat <- read.table("Particles_at_Catch_2012.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
rm(list=ls())
dat <- read.table("Particles_at_Catch_2013.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
rm(list=ls())
dat <- read.table("Particles_at_Catch_2014.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#10
rm(list=ls())
dat <- read.table("Particles_at_Catch_2015.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#NA
#Had to go back to the nest file used to make the model to get that there are 11 release points
rm(list=ls())
dat <- read.table("Particles_at_Catch_2016.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
rm(list=ls())
dat <- read.table("Particles_at_Catch_2017.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#6
rm(list=ls())
dat <- read.table("Particles_at_Catch_2018.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#10
rm(list=ls())
dat <- read.table("Particles_at_Catch_2019.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
DATES <- length(which(levels(dat$Date) %in% c("Aug1", "Aug15", "Jul1", "Jul15", "Jun1", "Jun15", "May1", "May15", "Oct1", "Sep1", "Sep15")))
DATES
#11
}



#Projecting the movement of the particles from a few locations


{```{R}```
#Libraries
suppressMessages(library(raster))
suppressMessages(library(rgdal))
suppressMessages(library(sf))
suppressMessages(library(spatialEco))
}

#Read shapefile
{```{R}```
polys <- readOGR(dsn="/home/afields/Workspace/Red_Snapper/analysis/Aug2019/model_larvae/support", layer = "GOM_Polygons_all")
polys <- spTransform(polys, CRS("+init=epsg:3160"))
}

#Prepare coordinates, data, and proj4string
{```{R}```
tmp_mat <- matrix(c(22.66201667, 19.9, 27.2385, 28.0356, 29.5105, 28.5335, 24.62913, -90.11301667, -96.283, -96.94333333, -91.5594, -88.076, -84.4868, -82.875655), ncol=2)
tmp_df <- data.frame(Lon=tmp_mat[,2], Lat=tmp_mat[,1])

tmp_loc <- data.frame(Location=c("Campeche, MX", "Veracruz, MX", "South TX, USA", "Central LA, USA", "Mobile Bay, USA", "Big Bend FL, USA", "Dry Tortugas FL, USA"))

coords <- gen.Gulf@strata[ , c('Lon','Lat')]
for(i in c('Lon','Lat')){coords[,i] <- as.numeric(as.matrix(coords[,i]))}
crs <- CRS("+init=epsg:3160")
}

# make the SpatialPointsDataFrame object
{```{R}```
pts <- SpatialPointsDataFrame(coords = tmp_df, data= tmp_loc, proj4string = crs)
}

#Look for intersection
{```{R}```
new_shape <- point.in.poly(pts, polys)

new_shape@data
{ #Results
              Location     N
1         Campeche, MX 19186
2         Veracruz, MX 24692
3        South TX, USA  8174
4      Central LA, USA  7233
5      Mobile Bay, USA  4083
6     Big Bend FL, USA  6307
7 Dry Tortugas FL, USA 14682
}

dput(new_shape@data$N)
{ #Results
c(19186L, 24692L, 8174L, 7233L, 4083L, 6307L, 14682L)
}
}

{```{R}```
#Libraries
library('ggmap')
library('adegenet')

#Parameter alterations
options("scipen"=100, "digits"=4)

#Read in Particle data
rm(list=ls())
dat <- read.table("Particles_at_Catch_2019.tab", head=T, col.names=c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N"))
#names(dat) <- c("Longitude", "Latitude", "Start", "Particle", "Time", "Depth", "Distance", "Exit_code", "Release_date", "Date", "N")
dat <- dat[!(is.na(dat$N)),]
dat$Start <- as.character(dat$Start)
head(dat)

Gulf_map <- ggmap(get_stamenmap(bbox = c(left = -100, bottom = 18, right =-74, top = 38.3), color="bw", force=T, zoom = 8, maptype=c("terrain-background")))
Gulf_map

Points <- c("19186", "24692", "8174", "7231", "4083", "6307", "14682")

Pts_dat <- dat[which(dat$Start %in% Points),c("Start","N")]
Pts_dat[,2] <- as.numeric(as.matrix(Pts_dat[,2]))

tmp_dat <- data.frame(matrix(nrow=nrow(Pts_dat),ncol=2))
names(tmp_dat) <- c("Lon", "Lat")
for(i in 1:nrow(Pts_dat)){
tmp_dat[i,] <- polys@polygons[[Pts_dat[i,"N"]]]@Polygons[[1]]@labpt
}

Pts_dat <- cbind(Pts_dat, tmp_dat)

tmp_df <- data.frame(matrix(nrow=length(Points),ncol=3))
names(tmp_df) <- c("Start", "Lon", "Lat")
tmp_df$Start <- as.numeric(Points)
for(i in 1:nrow(tmp_df)){
tmp_df[i,2:3] <- polys@polygons[[tmp_df[i,"Start"]]]@Polygons[[1]]@labpt
}
tmp_df$Loc <- c("Campeche, MX", "Veracruz, MX", "South TX, USA", "Central LA, USA", "Mobile Bay, USA", "Big Bend FL, USA", "Dry Tortugas FL, USA")

Gulf_map + geom_point(aes(x=Lon, y=Lat, color=Start), alpha=0.1, data=Pts_dat, pch=19) + scale_color_manual(values=Pts_col, name = "Release", labels = tmp_df$Loc[c(7,1,2,5,6,4,3)]) + geom_point(aes(x=Lon, y=Lat), data=tmp_df, pch=19, color="black") + guides(color = guide_legend(override.aes= list(alpha = 0.4)))

ggplot() + geom_point(aes(x=Lon, y=Lat, fill=Start), data=Pts_dat[which(Pts_dat$Start==14682),], pch=19) 

max(Pts_dat[Pts_dat$Start==14682]$Lon))

Gulf_map + geom_point(aes(x=Longitude, y=Latitude), data=subset(Pts_dat, Start==19186), pch=19) 
+ scale_fill_manual(values=Pts_col)


polys@polygons[[1]]@Polygons[[1]]@labpt


